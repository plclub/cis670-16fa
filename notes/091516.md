# Sept 15, 2016

- Don't forget your homework
- No class next week (but keep up with reading!)
- Send questions about Chapter 10 to both Solomon and me.

- Today: Type soundness 

# Questions

## Leading questions for today's lecture

- In the simple language E that we have developed so far, it seems that every
   well typed term e that evaluates to some e' will eventually evaluate to
   some value (canonical form). But I can imagine in a richer language we
   might have some well typed term that indeed progresses (evaluates to some
   e'), but it might never reach a canonical form (e' might evaluate back to e
   again in a few steps). In languages like these, it seems that big-step
   semantics fails to capture the "progress" that a divergent term e as
   mentioned above is making while the small-step semantics can capture it. Is
   this a concern? Is it possible to define the big-step semantics in some way
   so that it supports reasoning about this kind of behavior?

- I found section 7.3 on Type safety very interesting. I am unclear still
  about the relationship between progress, preservation, and type safety in
  evaluation dynamics. Could you go over this in class?

- If we consider the typing rules (4.1) as introduction rules for well-typed
  terms, then we can consider the evaluation context rules (5.7) as
  elimination rules for well-typed terms, with typing environments and
  evaluation contexts being the dual of each other, in some sense. We can also
  consider the instruction transition rules (5.5) as the elimination rules for
  the `inductive constructors`. So, the typing rules, in some sense, `make`
  the terms, while the transition rules of contextual (or structural) dynamics
  `breaks` the terms. Then the preservation property (6.1.1) of type safety
  can be paraphrased as break (make e) = make (break e), while the progress
  property (6.1.2) can be paraphrased as, if something exists, it is either
  `eternal`, or if it has been made, then it can be broken.


## But E is terminating! What about nonterminating programs?

- When we design a new language, should we design the static or dynamic first?
  Is there any dependent relation between static and dynamic?
  Moreover, if the program can always terminate(no loop), does it mean that
  all dynamic errors can be discovered statically? In other words, is
  termination the only factor that can decide whether the dynamic properties
  of the language can be proved statically?

- In Section 7.4: How often and useful are cost dynamics in practice? How do
  you define cost dynamics for arbitrary functions that may not terminate?

## What does syntax-directed actually mean?

- My question for today's class isn't about type safety per se, but about the
  term "syntax-directed". Does this term have a formal, agreed-upon
  definition?  In some parts of the book, "syntax directed" was in the sense
  that "there is exactly one rule for each form of expression" (e.g. Chapter
  4.2, Chapter 6.2). This makes sense because the syntax directs the rule to
  pick.

  On the other hand, in Chapter 7.1 with regards to the rules defining the
  evaluation dynamics: "The rules are not syntax directed, because the premise
  of Rule (7.1e) is not a subexpression of the expression in the conclusion of
  the rule". But syntax clearly directs the application of the rule here.

  This use of "syntax directed" is again noted in Chapter 7.2: "This induction
  principle is not the same as structural induction on e itself, because the
  evaluation rules are not syntax directed." It's clear that the two induction
  principles are different simply by noting they are defined in terms of
  different rules; but it seems like he's using "syntax directed" to actually
  mean recursing on subterms.

  Is there actually a formal, consistent meaning for "syntax directed"?

- In chapter 7 PFPL states that, referring to the big-step rules for
  evaluation in our language E, "This induction principle is not the same as
  structural induction on e itself, because the evaluation rules are not
  syntax-directed". Why aren't they? It seems like there's one rule for each
  form in the syntax of our language, so we should always know what rule we
  need to apply.

# Other questions

- In Ch5.v, we define determinacy as: if e steps to both e1 and e2, then e1
  and e2 are the same. However, in PFPL, Robert Harper defines it as: if e
  steps to both e1 and e2, e1 and e2 are alpha-equivalent. Why?

- Considering the structural similarity between opening bounded
  variables and substituting free variables, will it be meaningful that
  we unify the two kinds of variables and the corresponding operations
  into a more general concept?

